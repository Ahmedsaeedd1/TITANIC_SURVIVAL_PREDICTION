# -*- coding: utf-8 -*-
"""TITANIC SURVIVAL PREDICTION

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ti_df7N7G_4uqNC0_uZYGCBdV45ThOhA
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score, classification_report
# %matplotlib inline

df= pd.read_csv('TITANIC.csv')
df.head()

"""# Data preprocessing"""

def dataoveriew(df, message):
    print(f'{message}:\n')
    print("Rows:", df.shape[0])
    print("\nNumber of features:", df.shape[1])
    print("\nFeatures:")
    print(df.columns.tolist())
    print("\nMissing values:", df.isnull().sum().values.sum())
    print("\nUnique values:")
    print(df.nunique())

dataoveriew(df, 'Overiew of the training dataset')

#datatype inf
df.info()

# check the number of missing values in each column
df.isnull().sum()

# drop the "Cabin" column from the dataframe
df = df.drop(columns='Cabin', axis=1)

# replacing the missing values in "Age" column with mean value
df['Age'].fillna(df['Age'].mean(), inplace=True)

df['Fare'].fillna(df['Fare'].mean(), inplace=True)

df.isnull().sum()

#getting some statistical measures about the data
df.describe()

# finding the number of people survived and not survived
df['Survived'].value_counts()

"""# Data Visualization"""

df.describe().columns

#look at numerical and categrical values separtely
df_num= df[['Age','SibSp','Parch','Fare']]
df_cat= df[['Embarked','Pclass','Survived','Sex']]

for i in df_num:
  plt.hist(df_num[i])
  plt.title(i)
  plt.show()

print(df_num.corr())
sns.heatmap(df_num.corr())

#compare survival rate with age , sibsp , parch , fare
pd.pivot_table (df, index ='Survived' , values = ['Age','SibSp', 'Parch', 'Fare' ])

# making a count plot for "Survived" column
sns.countplot(x='Survived', data=df)

df['Sex'].value_counts()

#making a count plot for "Sex" column
sns.countplot(x='Sex', data=df)

# number of survivors Gender wise
sns.countplot(x='Sex', hue='Survived', data=df)

#making a count plot for "Pclass" column
sns.countplot(x='Pclass', data=df)

sns.countplot(x='Pclass', hue='Survived', data=df)

#making a count plot for "Pclass" column
sns.countplot(x='Embarked', data=df)

sns.countplot(x='Embarked', hue='Survived', data=df)

pd.pivot_table (df, index ='Survived' ,columns = 'Pclass', values = 'Ticket',aggfunc='count')

pd.pivot_table (df, index ='Survived' ,columns = 'Sex', values = 'Ticket',aggfunc='count')

pd.pivot_table (df, index ='Survived' ,columns = 'Embarked', values = 'Ticket',aggfunc='count')

plt.figure(figsize=(14,7))
plt.subplot(2,2,1)
sns.boxplot(x='Sex', y = 'Age',data= df)

plt.subplot(2,2,2)
sns.distplot(df['Fare'],color='g')

plt.subplot(2,2,3)
sns.distplot(df['Age'],color='b')


plt.tight_layout()
plt.show()

"""# correlation"""

# Heatmap
corrmat = df.corr()
fig = plt.figure(figsize = (12, 9))

sns.heatmap(corrmat, vmax = .8, square = True, annot = True)
plt.show()

"""# Encoding the Categorical Columns"""

df['Sex'].value_counts()

df['Embarked'].value_counts()

# converting categorical Columns
label_encoder = LabelEncoder()
df['Embarked'] = label_encoder.fit_transform(df['Embarked'])
df['Sex'] = label_encoder.fit_transform(df['Sex'])
#df.replace({'Sex':{'male':0,'female':1}, 'Embarked':{'S':0,'C':1,'Q':2}}, inplace=True)

df

"""# Separating features & Target"""

X = df.drop(columns = ['PassengerId','Name','Ticket','Survived'],axis=1)
Y = df['Survived']

print(X)

"""# Splitting the data into training data & Test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Model Building"""

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Instantiate the models
svm_model = SVC()
knn_model = KNeighborsClassifier()
logreg_model = LogisticRegression()
dt_model = DecisionTreeClassifier()

# Fit the models
svm_model.fit(X_train_scaled, Y_train)
knn_model.fit(X_train_scaled, Y_train)
logreg_model.fit(X_train_scaled, Y_train)
dt_model.fit(X_train_scaled, Y_train)

# Make predictions
svm_preds = svm_model.predict(X_test_scaled)
knn_preds = knn_model.predict(X_test_scaled)
logreg_preds = logreg_model.predict(X_test_scaled)
dt_preds = dt_model.predict(X_test_scaled)

# Calculate accuracy scores
svm_accuracy = accuracy_score(Y_test, svm_preds)
knn_accuracy = accuracy_score(Y_test, knn_preds)
logreg_accuracy = accuracy_score(Y_test, logreg_preds)
dt_accuracy = accuracy_score(Y_test, dt_preds)

# Generate classification reports
svm_report = classification_report(Y_test, svm_preds)
knn_report = classification_report(Y_test, knn_preds)
logreg_report = classification_report(Y_test, logreg_preds)
dt_report = classification_report(Y_test, dt_preds)

# Print the results
print("Support Vector Machine:")
print("Accuracy:", svm_accuracy)
print("Classification Report:\n", svm_report)
print("==================================")

print("K-Nearest Neighbors:")
print("Accuracy:", knn_accuracy)
print("Classification Report:\n", knn_report)
print("==================================")

print("Logistic Regression:")
print("Accuracy:", logreg_accuracy)
print("Classification Report:\n", logreg_report)
print("==================================")

print("Decision Tree:")
print("Accuracy:", dt_accuracy)
print("Classification Report:\n", dt_report)
print("==================================")

LR = LogisticRegression(solver='liblinear', max_iter=200)
LR.fit(X_train_scaled, Y_train)
cv = cross_val_score(LR ,X_train_scaled,Y_train , cv=5 )
print(cv)
print(cv.mean())
y_pred = LR.predict(X_test_scaled)
LRAcc = accuracy_score(Y_pred,Y_test)
print('Logistic regression accuracy: {:.2f}%'.format(LRAcc*100))

rf =RandomForestClassifier(random_state=1)
rf.fit(X_train_scaled, Y_train)
cv = cross_val_score(rf ,X_train_scaled,Y_train , cv=5 )
print(cv)
print(cv.mean())
y_pred = LR.predict(X_test_scaled)
rfAcc = accuracy_score(Y_pred,Y_test)
print('RandomForestClassifier accuracy: {:.2f}%'.format(LRAcc*100))